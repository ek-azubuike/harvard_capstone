---
title: "Harvard Data Science Capstone"
author: "Ekundayo Azubuike"
date: "2024-11-21"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE)
```

# Introduction

## Data Overview

The following analysis and modeling center on the `movielens` data set (you can read more about it [here](https://files.grouplens.org/datasets/movielens/ml-10m-README.html)). This `data.frame` is a subset of a larger data set containing 9,000,055 rows of observations and six columns of variables: `userId`, `movieId`, `rating`, `timestamp`, `title`, and `genres`. The target variable is `rating`, and the other five variables are predictors.

## Purpose

The goal of the analysis that follows is to build and train a statistical model to predict the `rating` for each row in the holdout set given a set of predictor variables. Root mean squared error (RMSE) will be the metric used to evaluate the quality of the model.

## Procedure

1.  First, I set up my environment by creating a new R project and populating the required files: `capstone_report.Rmd`, `capstone_report.pdf`, and `capstone_code.R`.
2.  Next, I loaded the `movielens` data according to the course instructions.
3.  I then performed some pre-processing to change a few data types, add new predictors, and split the data into training and test sets.
4.  I used the pre-processed data to perform exploratory data analysis to develop a preliminary understanding of the data.
5.  I performed variable selection and built a linear model of `rating` as a function of a subset of the original predictor variables: `movieId`, `userId`, `genres`, and `year`.
6.  I calculated regularized bias terms for each predictor.
7.  I calculated RMSE on the final model.
8.  Finally, I tested the model against the holdout set, yielding a final RMSE of less than `0.86549`.

# Methods/Analysis

## Loading Course Code

The following analysis will leverage functionality from the `tidyverse` library for data cleaning, manipulation, and visualization. I used `caret` for pre-processing and `Hmisc` for some exploratory data analysis.

```{r env_setup, message=F}

# install and load relevant libraries
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(Hmisc)) install.packages("Hmisc", repos = "http://cran.us.r-project.org")
if(!require(dslabs)) install.packages("dlabs", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(Hmisc)
library(dslabs)
library(knitr)
ds_theme_set()

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

options(timeout = 120)

dl <- "ml-10M100K.zip"
if(!file.exists(dl))
  download.file("https://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings_file <- "ml-10M100K/ratings.dat"
if(!file.exists(ratings_file))
  unzip(dl, ratings_file)

movies_file <- "ml-10M100K/movies.dat"
if(!file.exists(movies_file))
  unzip(dl, movies_file)

ratings <- as.data.frame(str_split(read_lines(ratings_file), fixed("::"), simplify = TRUE),
                         stringsAsFactors = FALSE)
colnames(ratings) <- c("userId", "movieId", "rating", "timestamp")
ratings <- ratings %>%
  mutate(userId = as.integer(userId),
         movieId = as.integer(movieId),
         rating = as.numeric(rating),
         timestamp = as.integer(timestamp))

movies <- as.data.frame(str_split(read_lines(movies_file), fixed("::"), simplify = TRUE),
                        stringsAsFactors = FALSE)
colnames(movies) <- c("movieId", "title", "genres")
movies <- movies %>%
  mutate(movieId = as.integer(movieId))

movielens <- left_join(ratings, movies, by = "movieId")

# Final hold-out test set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.6 or later
# set.seed(1) # if using R 3.5 or earlier
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in final hold-out test set are also in edx set
final_holdout_test <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from final hold-out test set back into edx set
removed <- anti_join(temp, final_holdout_test)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

## Inspecting Data

The data is contained within a `data.frame` object, and it contains three integer vectors (`userId`, `movieId`, and `timestamp`), two character vectors (`title` and `genres`), and a target vector of doubles (`rating`). The `summary()` doesn't provide a very useful overview because many of the variables are not properly encoded, but we do get a somewhat useful understanding of the distribution of the target variable `rating` given the measures of centrality. The mean and median values for `rating` are `3.512` and `4.000` respectively.

```{r}
# view summary of edx data set
str(edx)
class(edx)
summary(edx) %>% kable()
head(edx) %>% kable()
```

## Data Pre-processing

As a first step, I created a `year` column by using a regular expression (`"\\((\\d{4})\\)$"`) to match the characters between the parentheses at the end of the `title` string and converting the match to a factor. Similarly, I converted the `userId`, `movieId`, and `genres` columns to factors as well for analysis. For the `genres` column, I also extracted the primary categorization with a regular expression (`"^[^|]+"`) to reduce the number of levels in the factor for analysis. Lastly, I converted the `timestamp` column to a datetime and reviewed the summary with the recoded values. Already, it's clear that some users have supplied more ratings than others. Moreover, some movies receive more ratings than others. `Action`, `Comedy`, and `Drama` are the genres with the greatest number of reviews. Lastly, different years have different numbers of reviews, with `1995` having the greatest number. All of this information points to the idea that these four variables (`userId`, `movieId`, `genres`, and `year`) may benefit from regularization to account for the differences in numbers of ratings per category. The `date` column ranges from `29 January 1996` to `5 January 2009`.

```{r}
# recode variables and view summary
edx.mutated <- edx %>% 
  mutate(year = factor(str_match(edx$title, "\\((\\d{4})\\)$")[,2]),
         date = as_datetime(timestamp),
         userId = factor(userId),
         movieId = factor(movieId),
         genres = factor(str_match(edx$genres, "^[^|]+")))

summary(edx.mutated) %>% kable()
head(edx.mutated) %>% kable()
```

## Training and Testing Subsets

I created a 20% partition on the target variable `rating` to generate a testing index. I used that index to subset the original data so that I could trian my model on 80% of the original data and test it on the remaining 20%. I used the `semi_join()` to ensure that all movies represented in the training set were also present in the testing set.

```{r}
# partition testing and training sets
test.index <- createDataPartition(edx.mutated$rating, 
                                  p = 0.2, 
                                  list = F)
edx.test <- edx.mutated[test.index, ]
edx.train <- edx.mutated[-test.index, ]

# ensure that all movies in training set are also in testing set
edx.test <- edx.test %>% 
  semi_join(edx.train, by = "userId")
edx.train <- edx.train %>% 
  semi_join(edx.test, by = "userId")

```

## Exploratory Data Analysis

### Predictor Correlation

I began my exploratory data analysis by computing a Spearman correlation matrix to determine if any of the variables were too highly correlated. The highest correlation (`0.50`) exists between the `movieId` and `timestamp` variables, but the correlation isn't very strong, so it shouldn't affect analysis.

```{r}
# correlation matrix
edx.train %>% 
  select(-c(rating, genres, title, date)) %>% 
  as.matrix() %>% 
  Hmisc::rcorr(type = "spearman")
```

### Stratification, Summary, and Visualization

Next, I stratified the data by each categorical predictor (`movieId`, `userId`, `genres`, and `year`) in order to uncover patterns in the distribution of rating counts and measures of centrality of the rating (mean and standard deviation).

#### Stratification by `movieId`

As there is a large range of number of reviews (`25114`), I am justified in applying regularization to the `movieId` bias term later on in the analysis to lessen the impact of movies with fewer reviews on the predictions.

```{r}
# examine the distribution of ratings by movie
edx.train %>% 
  group_by(movieId) %>% 
  summarise(movie.avg = mean(rating),
            movie.sd = sd(rating),
            rating.count = n()) %>% 
  arrange(desc(rating.count)) %>% 
  slice(1:10) %>% 
  kable()

edx.train %>% 
  group_by(movieId) %>% 
  summarise(movie.avg = mean(rating),
            rating.count = n()) %>% 
  summarise(range = max(rating.count) - min(rating.count)) %>% 
  kable()
```

A histogram of the distribution of rating counts among movies further illustrates the point above: it is more common for movies to have 2,000 or fewer reviews. The distribution is not normal; it has positive skew (that is, its right tail is longer due to the higher prevalence of smaller values).

```{r}
# histogram of the distribution of rating counts among movies
edx.train %>% 
  group_by(movieId) %>% 
  summarise(movie.avg = mean(rating),
            rating.count = n()) %>%
  ggplot(aes(x = rating.count)) +
  geom_histogram(binwidth = 1000) +
  scale_y_log10() +
  ggtitle("Distribution of Count of Ratings among Movies") +
  xlab("Number of Ratings") +
  ylab("Count")
```

#### Stratification by `userId`

Stratification by `userId` reveals a similar but less dramatic range in rating counts between users (`5317`) that justifies a regularization procedure on the `userId` bias term.

```{r}
# examine the distribution of ratings by user
edx.train %>% 
  group_by(movieId) %>% 
  summarise(user.avg = mean(rating),
            user.sd = sd(rating),
            rating.count = n()) %>% 
  arrange(desc(rating.count)) %>%
  slice(1:10) %>% 
  kable()

edx.train %>% 
  group_by(userId) %>% 
  summarise(user.avg = mean(rating),
            rating.count = n()) %>% 
  summarise(range = max(rating.count) - min(rating.count)) %>% 
  kable()
```

The histogram of rating counts among users also has positive skew, indicating that smaller values predominate.

```{r}
# histogram of the distribution of rating counts among users
edx.train %>% 
  group_by(userId) %>% 
  summarise(user.avg = mean(rating),
            ratings.count = n()) %>%
  ggplot(aes(x = ratings.count)) +
  geom_histogram(binwidth = 100) +
  scale_y_log10() +
    ggtitle("Distribution of Count of Ratings among Users") +
  xlab("Number of Ratings") +
  ylab("Count")
```

#### Stratification by `genres`

Stratification by `genres` indicates that `Action` movies are the most frequently rated, whereas `Film-Noir` movies receive the highest rating on average. A large range of rating counts between genres (`2047641`) justifies a regularization procedure on the `genres` bias term.

```{r}
# counts, average ratings, and range by genre
edx.train %>% 
  group_by(genres) %>% 
  summarise(genre.avg = mean(rating),
            count = n()) %>% 
  arrange(desc(count)) %>%
  kable()

edx.train %>% 
  group_by(genres) %>% 
  summarise(genre.avg = mean(rating),
            count = n()) %>% 
  arrange(desc(genre.avg)) %>% 
  kable()

edx.train %>% 
  group_by(genres) %>% 
  summarise(genre.avg = mean(rating),
            rating.count = n()) %>% 
  summarise(range = max(rating.count) - min(rating.count)) %>% 
  kable()
```

A bar graph illustrates the stark differences in rating counts between genres.

```{r}
# bar graph of rating counts by genre
edx.train %>% 
  group_by(genres) %>% 
  summarise(genre.avg = mean(rating),
            rating.count = n()) %>%
  ggplot(aes(x = reorder(genres, desc(rating.count)), y = rating.count)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(title = "Rating Counts by Genre") +
  xlab("Genre") +
  ylab("Count of Ratings")
```

#### Stratification by `year`

Stratification by `year` highlights two points. First, movies that were released in 1946 have the highest average rating. Second, movies released in the 1990's consistently have the highest number of ratings. The range in number of ratings between the most- and least-rated years is `629022`, justifying a regularization procedure on the `year` bias term.

```{r}
# counts, average ratings, and range by year
edx.train %>% 
  group_by(year) %>% 
  summarise(year.avg = mean(rating),
            rating.count = n()) %>% 
  arrange(desc(year.avg)) %>% 
  head(20) %>% 
  kable()

edx.train %>% 
  group_by(year) %>% 
  summarise(year.avg = mean(rating),
            rating.count = n()) %>% 
  arrange(desc(year.avg)) %>% 
  tail(20) %>% 
  kable()

edx.train %>% 
  group_by(year) %>% 
  summarise(year.avg = mean(rating),
            rating.count = n()) %>% 
  arrange(desc(rating.count)) %>% 
  slice(1:20) %>% 
  kable()

edx.train %>% 
  group_by(year) %>% 
  summarise(year.avg = mean(rating),
            rating.count = n()) %>% 
  summarise(range = max(rating.count) - min(rating.count)) %>% 
  kable()
```

A bar graph of rating counts by year clearly illustrates the higher frequency of ratings among movies released in the 1990's.

```{r}
# bar graph of rating counts by year
edx.train %>% 
  group_by(year) %>% 
  summarise(year.avg = mean(rating),
            rating.count = n()) %>%
  ggplot(aes(x = year, y = rating.count)) +
  geom_bar(stat = "identity") +
  scale_x_discrete(breaks = seq(1915, 2008, 5)) +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(title = "Rating Counts by Year") +
  xlab("Year") +
  ylab("Count of Ratings")
```

## Bias Term Regularization

### Setup and Overview

The next step was to calculate regularized bias terms for the selected predictors (`movieId`, `userId`, `genres`, and `year`). I first stored the overall mean (`mu`) and test ranges for `lambda` in variables. I ultimately ran my code using the variable `even.more.lambdas` after testing other ranges. The benefit of the structure of `even.more.lamdbas` is that exponentiation allowed me to explore a wide range of values for `lambda` with a higher level of granularity as lower values.

```{r}
# regularization procedures: define overall average `mu` and bias penalty ranges to test
mu <- mean(edx.train$rating)
lambdas <- seq(0, 20, 0.1)
more.lambdas <- seq(0, 100, 5)
even.more.lambdas <- 10^seq(-2, 2, 0.1)
```

Regularization followed the same general procedure for each term. For the sake of brevity, I have summarized the procedure here:

1.  Subtract the overall mean (`mu`) from each `rating` to calculate the difference.
2.  Stratify by the predictor in question.
3.  Determine the number of ratings in each stratum.
4.  Calculate the sum of the difference between the mean (`mu`) and each `rating`.
5.  Test a range of values for `lambda` (the regularization term).
6.  Select the value for `lambda` that minimizes RMSE.
7.  Store the regularized bias terms with the ideal value for `lambda` in a variable for future modeling.

### Movie Bias Term Regularization

```{r}
# find ideal lambda for movie regularization
movie.rmses <- sapply(even.more.lambdas, function(x) {
  edx.train %>% 
    mutate(mu = mean(rating),
           y.mu.diff = rating - mu) %>% 
    group_by(movieId) %>% 
    summarise(n.movies = n(),
              sum.movie.diff = sum(y.mu.diff),
              b_lambda = sum.movie.diff / (n.movies + x)) %>% 
    left_join(edx.test, by = "movieId") %>% 
    mutate(pred = mu + b_lambda) %>% 
    filter(!is.na(rating)) %>% 
    summarise(rmse = RMSE(pred, rating)) %>% 
    pull(rmse)
})

# cbind(lambdas, movie.rmses)
# cbind(more.lambdas, movie.rmses)
cbind(even.more.lambdas, movie.rmses) %>% kable()
l.movie <- even.more.lambdas[which.min(movie.rmses)]

# movie regularization
edx.train %>% 
  mutate(mu = mean(rating),
         y.mu.diff = rating - mu) %>% 
  group_by(movieId) %>% 
  summarise(n.movies = n(),
            sum.movie.diff = sum(y.mu.diff),
            b_lambda = sum.movie.diff / (n.movies + l.movie)) %>% 
  ungroup() %>% 
  left_join(edx.test, by = "movieId") %>% 
  mutate(pred = mu + b_lambda) %>% 
  filter(!is.na(rating)) %>%
  summarise(rmse = RMSE(pred, rating)) %>% 
  kable()

b.movie <- edx.train %>% 
  mutate(mu = mean(rating),
         y.mu.diff = rating - mu) %>% 
  group_by(movieId) %>% 
  summarise(n.movies = n(),
            sum.movie.diff = sum(y.mu.diff),
            b_movie = sum.movie.diff / (n.movies + l.movie)) %>% 
  ungroup()
```

The ideal `lambda` for regularization of the `movieId` bias term is `r l.movie`.

### User Bias Term Regularization

```{r}
# find ideal lambda for user regularization
user.rmses <- sapply(even.more.lambdas, function(x) {
  edx.train %>% 
    left_join(b.movie, by = "movieId") %>% 
    mutate(mu = mean(rating),
           y.mu.diff = rating - mu - b_movie) %>% 
    group_by(userId) %>% 
    summarise(n.users = n(),
              sum.user.diff = sum(y.mu.diff),
              b_lambda = sum.user.diff / (n.users + x)) %>% 
    left_join(edx.test, by = "userId") %>% 
    mutate(pred = mu + b_lambda) %>% 
    filter(!is.na(rating)) %>% 
    summarise(rmse = RMSE(pred, rating)) %>% 
    pull(rmse)
})

cbind(even.more.lambdas, user.rmses) %>% kable()
l.user <- even.more.lambdas[which.min(user.rmses)]

# user regularization
edx.train %>% 
  left_join(b.movie, by = "movieId") %>% 
  mutate(mu = mean(rating),
         y.mu.diff = rating - mu - b_movie) %>% 
  group_by(userId) %>%
  summarise(n.users = n(),
            sum.user.diff = sum(y.mu.diff),
            b_lambda = sum.user.diff / (n.users + l.user)) %>%
  left_join(edx.test, by = "userId") %>%
  mutate(pred = mu + b_lambda) %>%
  filter(!is.na(rating)) %>%
  summarise(rmse = RMSE(pred, rating)) %>% 
  kable()

b.user <- edx.train %>% 
  left_join(b.movie, by = "movieId") %>% 
  mutate(mu = mean(rating),
         y.mu.diff = rating - mu - b_movie) %>% 
  group_by(userId) %>% 
  summarise(n.users = n(),
            sum.user.diff = sum(y.mu.diff),
            b_user = sum.user.diff / (n.users + l.user)) %>% 
  ungroup()
```

The ideal `lambda` for regularization of the `userId` bias term is `r l.user`.

### Genre Bias Term Regularization

```{r}
# find ideal lambda for genre regularization
genre.rmses <- sapply(even.more.lambdas, function(x) {
  edx.train %>% 
    left_join(b.movie, by = "movieId") %>% 
    left_join(b.user, by = "userId") %>% 
    mutate(mu = mean(rating),
           y.mu.diff = rating - mu - b_movie - b_user) %>% 
    group_by(genres) %>% 
    summarise(n.genres = n(),
              sum.genre.diff = sum(y.mu.diff),
              b_lambda = sum.genre.diff / (n.genres + x)) %>% 
    left_join(edx.test, by = "genres") %>% 
    mutate(pred = mu + b_lambda) %>% 
    filter(!is.na(rating)) %>% 
    summarise(rmse = RMSE(pred, rating)) %>% 
    pull(rmse)
})

cbind(even.more.lambdas, genre.rmses) %>% kable()
l.genre <- even.more.lambdas[which.min(genre.rmses)]

# genre regularization
edx.train %>% 
  left_join(b.movie, by = "movieId") %>% 
  left_join(b.user, by = "userId") %>% 
  mutate(mu = mean(rating),
         y.mu.diff = rating - mu - b_movie - b_user) %>% 
  group_by(genres) %>% 
  summarise(n.genres = n(),
            sum.genre.diff = sum(y.mu.diff),
            b_lambda = sum.genre.diff / (n.genres + l.genre)) %>% 
  left_join(edx.test, by = "genres") %>% 
  mutate(pred = mu + b_lambda) %>% 
  filter(!is.na(rating)) %>% 
  summarise(rmse = RMSE(pred, rating)) %>% 
  kable()

b.genre <- edx.train %>% 
  left_join(b.movie, by = "movieId") %>% 
  left_join(b.user, by = "userId") %>% 
  mutate(mu = mean(rating),
         y.mu.diff = rating - mu - b_movie - b_user) %>% 
  group_by(genres) %>% 
  summarise(n.genres = n(),
            sum.genre.diff = sum(y.mu.diff),
            b_genre = sum.genre.diff / (n.genres + l.genre)) %>% 
  ungroup()
```

The ideal `lambda` for regularization of the `genres` bias term is `r l.genre`.

### Year Bias Term Regularization

```{r}
# find ideal lambda for year regularization
year.rmses <- sapply(even.more.lambdas, function(x) {
  edx.train %>% 
    left_join(b.movie, by = "movieId") %>% 
    left_join(b.user, by = "userId") %>% 
    left_join(b.genre, by = "genres") %>% 
    mutate(mu = mean(rating),
           y.mu.diff = rating - mu - b_movie - b_user - b_genre) %>% 
    group_by(year) %>% 
    summarise(n.year = n(),
              sum.year.diff = sum(y.mu.diff),
              b_lambda = sum.year.diff / (n.year + x)) %>% 
    left_join(edx.test, by = "year") %>% 
    mutate(pred = mu + b_lambda) %>% 
    filter(!is.na(rating)) %>% 
    summarise(rmse = RMSE(pred, rating)) %>% 
    pull(rmse)
})

cbind(even.more.lambdas, year.rmses) %>% kable()
l.year <- even.more.lambdas[which.min(year.rmses)]

# year regularization
edx.train %>% 
  left_join(b.movie, by = "movieId") %>% 
  left_join(b.user, by = "userId") %>%
  left_join(b.genre, by = "genres") %>% 
  mutate(mu = mean(rating),
         y.mu.diff = rating - mu - b_movie - b_user - b_genre) %>% 
  group_by(year) %>% 
  summarise(n.year = n(),
            sum.year.diff = sum(y.mu.diff),
            b_year = sum.year.diff / (n.year + l.year)) %>% 
  left_join(edx.test, by = "year") %>% 
  mutate(pred = mu + b_year) %>% 
  filter(!is.na(rating)) %>% 
  summarise(rmse = RMSE(pred, rating)) %>% 
  kable()

b.year <-edx.train %>% 
  left_join(b.movie, by = "movieId") %>% 
  left_join(b.user, by = "userId") %>%
  left_join(b.genre, by = "genres") %>% 
  mutate(mu = mean(rating),
         y.mu.diff = rating - mu - b_movie - b_user - b_genre) %>% 
  group_by(year) %>% 
  summarise(n.year = n(),
            sum.year.diff = sum(y.mu.diff),
            b_year = sum.year.diff / (n.year + l.year)) %>% 
  ungroup()
```

The ideal `lambda` for regularization of the `year` bias term is `r l.year`.

# Results

## Model Evaluation

### Naive Model ("Guessing")

I started by modeling random guesses for the `rating` variable by sampling from a discrete uniform distribution between 0.5 and 5 inclusive with replacement. Random guessing yields a RMSE of greater than `1.9`. This will serve as our baseline for evaluation.

```{r}
# naive model ("guessing")
guesses <- sample(x = seq(0.5, 5, 0.5),
                  size = nrow(edx.test),
                  replace = T)
RMSE(guesses, edx.test$rating)
```

### Movie Bias Model

After adding in a regularized bias term for `movieId`, we get a significantly improves RMSE of `0.9436776`.

```{r}
edx.test %>% 
  left_join(b.movie, by = "movieId") %>% 
  mutate(pred = mu + b_movie) %>% 
  filter(!is.na(pred)) %>% 
  summarise(rmse = RMSE(rating, pred)) %>% 
  kable()
```

### Movie + User Bias Model

Including a regularized bias term for `userId` further improves the RMSE of our predictions to `0.865678`.

```{r}
edx.test %>% 
  left_join(b.movie, by = "movieId") %>% 
  left_join(b.user, by = "userId") %>%
  mutate(pred = mu + b_movie + b_user) %>% 
  filter(!is.na(pred)) %>% 
  summarise(rmse = RMSE(rating, pred)) %>%
  kable()
```

### Movie + User + Genre Bias Model

Adding the regularized bias term for `genres` provides a very small improvement to RMSE: `0.8655764`.

```{r}
edx.test %>% 
  left_join(b.genre, by = "genres") %>% 
  left_join(b.movie, by = "movieId") %>% 
  left_join(b.user, by = "userId") %>% 
  mutate(pred = mu + b_movie + b_user + b_genre) %>% 
  filter(!is.na(pred)) %>% 
  summarise(rmse = RMSE(rating, pred)) %>% 
  kable()
```

### Movie + User + Genre + Year Bias Model

Adding the final regularized bias term for `year` improves our RMSE by another small margin: `0.8653371`

```{r}
edx.test %>% 
  left_join(b.year, by = "year") %>% 
  left_join(b.genre, by = "genres") %>% 
  left_join(b.movie, by = "movieId") %>% 
  left_join(b.user, by = "userId") %>% 
  mutate(pred = mu + b_movie + b_user + b_genre + b_year) %>% 
  filter(!is.na(pred)) %>% 
  summarise(rmse = RMSE(rating, pred)) %>% 
  kable()
```

Our final model is as follows: $rating = \mu + b_{movie}(\lambda) + b_{user}(\lambda) + b_{genre}(\lambda) + b_{year}(\lambda)$ where `mu` is the overall average of `rating` in the training set, and `b_predictor(lambda)` are the regularized bias terms for each of the selected predictors.

## Final Model Performance

When tested against the pre-processed `final_holdout_test` set, the model yields a final RMSE of `0.8653659` (less than 0.86549).

```{r}
# final testing
final_holdout_test %>% 
  mutate(year = factor(str_match(final_holdout_test$title, "\\((\\d{4})\\)$")[,2]),
         genres = factor(str_match(final_holdout_test$genres, "^[^|]+")),
         userId = factor(userId),
         movieId = factor(movieId)) %>% 
  left_join(b.year, by = "year") %>% 
  left_join(b.genre, by = "genres") %>% 
  left_join(b.movie, by = "movieId") %>% 
  left_join(b.user, by = "userId") %>%
  mutate(pred = mu + b_movie + b_user + b_genre + b_year) %>% 
  filter(!is.na(pred)) %>% 
  summarise(final.rmse = RMSE(rating, pred)) %>% 
  kable()
```

# Conclusion

## Summary

In conclusion, a predictive model for `ratings` that includes regularized bias terms for the `movieId`, `userId`, `genres`, and `year` predictors yielded the lowest RMSE and was therefore the best model. The model can be represented with the following equation: $rating = \mu + b_{movie}(\lambda) + b_{user}(\lambda) + b_{genre}(\lambda) + b_{year}(\lambda)$

## Limitations and Future Work

Due to memory (RAM) limitations, some of the `caret` library's more powerful models were not available to me as options. With greater processing power, I would like to compare the results of the model I have developed to those of a k-nearest neighbors classification model and a random forest model. Moreover, I would like to implement k-folds cross-validation to identify the ideal tuning parameters for each of the models (e.g. `k` and `mtry` respectively).
